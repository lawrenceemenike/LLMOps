{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZukBRANA7pbb"
      },
      "outputs": [],
      "source": [
        "# Import Vertex AI\n",
        "\n",
        "import vertexai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project = PROJECT_ID,\n",
        "              location = REGION,\n",
        "              credentials = credentials)"
      ],
      "metadata": {
        "id": "mVyKnnz37sXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BigQuery to use as data warehouse\n",
        "\n",
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "i8s8CjAQ70na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the client to interact with data warehouse\n",
        "\n",
        "bq_clent = bigquery.CLient(project=PROJECT_ID,\n",
        "                           credentials = credentials)"
      ],
      "metadata": {
        "id": "2rurtGMT728T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to retrieve table_name of all the TABLES\n",
        "QUERY_TABLES = \"\"\"\n",
        "SELECT\n",
        "  table_name\n",
        "FROM\n",
        "  'bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LPYp7CEJ8BBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the data\n",
        "\n",
        "query_job = bq_client.query(QUERY_TABLES)"
      ],
      "metadata": {
        "id": "ioSzI7kG9EBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in query_job:\n",
        "  for value in row.values():\n",
        "    print(value)"
      ],
      "metadata": {
        "id": "aVInxZbR9ViT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch data from data warehouse and store in Pandas Dataframe\n",
        "\n",
        "INSPECT_QUERY = \"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `bigquery-public-data.stackoverflow.posts_questions`\n",
        "LIMIT 3\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j1-DKzt19dtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas pd\n",
        "\n",
        "query_job - bq_client.query(INSPECT_QUERY)"
      ],
      "metadata": {
        "id": "iwPcEOw-9udA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_overflowd_df = query_job\\\n",
        "  .result()\\\n",
        "  .to_arrow()\\\n",
        "  .to_pandas()\n",
        "stack_overflow_df.head()"
      ],
      "metadata": {
        "id": "cgMHTqpg95gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query dealing with large Datasets\n",
        "Query_ALL = \"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  'bigquery-public-data.stackoverflow.posts_questions' q\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5qyoKKaQ-F-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_job = bq_client.query(QUERY_ALL)\n",
        "\n",
        "try:\n",
        "  stack_overflow_df = query_job\\\n",
        "  .result()\\\n",
        "  .to_arrow()\\\n",
        "  .to_pandas()\n",
        "except Exception as e:\n",
        "  print('The DataFrame is too large to load into memory.', e)"
      ],
      "metadata": {
        "id": "yU4I3xnw-aaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining Tables and Query Optimization\n",
        "\n",
        "QUERY = \"\"\"\n",
        "SELECT\n",
        "    CONCAT(q.title, q.body) as input_text,\n",
        "    a.body AS output_text\n",
        "FROM\n",
        "    'bigquery-public-data.stackoverflow.posts_questions' q\n",
        "JOIN\n",
        "    'bigquery-public-data.stackoverflow.posts_answers' a\n",
        "ON\n",
        "    q.accepted_answer_id = a.id\n",
        "WHERE\n",
        "    q.accepted_answer_id IS NOT NULL AND\n",
        "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
        "    a.creation_data >= \"2020-01-01\n",
        "LIMIT\n",
        "  1000\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ebSieEa5-1ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_job = bq_client.query(QUERY)\n"
      ],
      "metadata": {
        "id": "TaZj4nA2_s3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### this may take some seconds to run\n",
        "stack_overflow_df = query_job.result()\\\n",
        "                    .to_arrow()\\\n",
        "                    .to_pandas()\n",
        "stack_overflow_df.head(2)"
      ],
      "metadata": {
        "id": "OQjVV9Zc_23Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding instructions to improve LLM performance\n",
        "\n",
        "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
        "Please answer the following Stackoverflow question on Python. \\\n",
        "Answer it like you are a developer answering Stackoveflow questions.\n",
        "\n",
        "Stackoverflow quesstion:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cf-1tC8vAKbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Instruction Template and question input_text\n",
        "\n",
        "stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' '\\ + stackk_overflow_df['input_text']"
      ],
      "metadata": {
        "id": "EbPRbj1EAjqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into a training and evaluation set using 80/20 split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, evaluation = train_test_split(\n",
        "    stack_overflow_df,\n",
        "    ### test-size=0.2 means 20% for evaluation\n",
        "    ## which then makes train set to be of 80%\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "dHp_3ONzBpcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Versioning data for reprodcuobiloity, tracebility and mainttainbility\n",
        "\n",
        "import datatime\n",
        "\n",
        "date = datetime.datetime.now().strftime(%H:%d:%m:%Y)"
      ],
      "metadata": {
        "id": "eec83dAkCE_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate jsonl file\n",
        "\n",
        "cols = ['input_text_instruct', 'output_text']\n",
        "tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)\n",
        "\n",
        "training_data_filename = f\"tune_data-stack_overflow_\\\n",
        "                          python_qa-{date}.jsonl\"\n"
      ],
      "metadata": {
        "id": "pxrSbMvlCYXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(training_data_filename, \"w\") as f:\n",
        "  f.write(tine_jsonl)"
      ],
      "metadata": {
        "id": "SP3wJ96JC4ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "cols = ['input_text_instruct', 'output_text']\n",
        "### you need to use the 'evaluation' set now\n",
        "tune_jsonl = evaluation[cols].to_json(orient='records', lines=True)\n",
        "\n",
        "### change the file name\n",
        "### use 'tune_eval_data_stack_overflow_python_qa-{date}.jsonl\n",
        "evaluation_data_filename = f\"tune_eval_data_stack_overflow_z\\\n",
        "                            python_qa-{date}.jsonl\"\n",
        "\n",
        "### write the file\n",
        "with open(evaluation_data_filename, \"w\") as f:\n",
        "  f.write(tune_jsonl)"
      ],
      "metadata": {
        "id": "hBem0j9REfSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp import dsl\n",
        "from kfp import complier\n",
        "\n",
        "# Ignore FutureWarnings in kfp\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\".,\n",
        "                        category=FutureWarning,\n",
        "                        modelule='kfp,*')"
      ],
      "metadata": {
        "id": "SuA8hVt5JYW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple example: component 1\n",
        "@dsl.component\n",
        "def say_hello(name, str) -> str:\n",
        "  hello_text = f'Hello, {name}!'\n",
        "\n",
        "  return hello_text"
      ],
      "metadata": {
        "id": "YQbvYnEaJp8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hello_task = say_hello(names=\"Erwin\")\n",
        "print(hello_task)"
      ],
      "metadata": {
        "id": "cIAArL5kKPlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hello_task.output)"
      ],
      "metadata": {
        "id": "solo0hF-K99F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will give an error and ask you to specify the parameter name\n",
        "hello_task = say_hello9(\"Erwin\")"
      ],
      "metadata": {
        "id": "hleXF4uVLDcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple example: component 2\n",
        "@dsl.component\n",
        "def how-are_you(hello_text: str) -> str:\n",
        "\n",
        "  how_are_you = f\"{hello_text}. How are your?\"\n",
        "\n",
        "  return how_are_you"
      ],
      "metadata": {
        "id": "ZjQo93vtLDnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how_task = how-are_you(hello_text=hello.output)\n",
        "print(how_task)\n",
        "print(how_task.output)"
      ],
      "metadata": {
        "id": "FbcSr9t-LqEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will give an error and ask you to pass in a built-in data type\n",
        "how_task = how_are_you(hello_text=hello_task)\n",
        "print(how_task)\n",
        "print(how_teask.output)"
      ],
      "metadata": {
        "id": "uRfXNPYiLD4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple example: pipeline\n",
        "@dsl.pipeline\n",
        "def hello_pipeline(receipent: str) -> str:\n",
        "\n",
        "  # notice, just recipient and not recipeient.output\n",
        "  hello_task = say_hello(name=recipient)\n",
        "\n",
        "  # notice.output\n",
        "  how_task = how_are_you(hello_text=hello_task.output)\n",
        "\n",
        "  # notice .output\n",
        "  return how_task.output"
      ],
      "metadata": {
        "id": "XSJcmVOFMGSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_output = hello_pipeline(recipient=\"Erwin\")\n",
        "print(pipeline_output)"
      ],
      "metadata": {
        "id": "evb1bZlgMGlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Pipeline with wrong return value type\n",
        "@dsl.pipeline\n",
        "def hello_pipeline_with_eror(recipient: st) -> str:\n",
        "  hello_taks = say_hello(name=recipient)\n",
        "  how_task = how_are_you(hello_text=hello_task.output)\n",
        "\n",
        "  return how_task"
      ],
      "metadata": {
        "id": "5iEO95H-NTaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impment the pipeline\n",
        "\n",
        "*   A pipeline is a set of componets that you orcestrate\n",
        "*   It lets you define the order of execution and how data flows from one step to another\n",
        "*   compile the pipeline into a yaml file, `pipeline.yml`\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a-xrtqfgODLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')"
      ],
      "metadata": {
        "id": "MD4YxITpOoyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_arguments = {\n",
        "    \"recipient\": \"World!\",\n",
        "}"
      ],
      "metadata": {
        "id": "aDoE4IR1PB52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat pipeline.yaml"
      ],
      "metadata": {
        "id": "s-IdiXECPQJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### import `PipelineJob`\n",
        "from google.aiplatform import PipelineJob\n",
        "\n",
        "job = PipelineJob(\n",
        "    ### path of the yaml file to execute\n",
        "    template_path =\"pipeline.yaml\",\n",
        "    ## name of the pipeline\n",
        "    display_name=f\"deep_learning_ai_pipeline\",\n",
        "    ### pipeline arguements (inputs)\n",
        "    ### {\"receipent\": \"World!\"} for this example\n",
        "    parameters_values=pipeline-arguments,\n",
        "    ### region of execution\n",
        "    location=\"us-centrail1\"\n",
        "    ### root is where temporary files are being\n",
        "    ### stored by the execution engine\n",
        "    pipeline_root =\"./\",\n",
        ")\n",
        "\n",
        "### submit for execution\n",
        "job.submit()\n",
        "\n",
        "### check to see the status of the job\n",
        "job.state"
      ],
      "metadata": {
        "id": "M53apDJEPlaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-life Example\n",
        "\n",
        "## Automation and Orchestration of a Supervised Tuning Pipeline\n",
        "\n",
        "\n",
        "*   Reuse an exisitng Kubeflow Pipeline for Parameter-Efficient Fine-Tuning (PEFT) for a foundation model from Google, called PaLM 2\n",
        "*   Advantage of reusing a pipeline means you do not have to build it from scratch, you can only specify some of the parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "BI0pzLd3Q7Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DATA_URI = \"./tune_data_stack_overflow_python_qa.jsonl\"\n",
        "EVALUATION_DATA_URI = \"./tune_eval_data_stack_overflow_python_qa.jsonl\""
      ],
      "metadata": {
        "id": "qOgJtN0lRX1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Provide the model with a version\n",
        "*   Versioning model allows for:\n",
        "    *   Reproducibility: Reproduce your results and ensure your models perform as expected\n",
        "    *   Auditing: Track changes to your models\n",
        "    *   Rollbacks: Roll back to a previous version of your model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_-0F-YSARr2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### path to the pipeline file to reuse\n",
        "## the file is provided in your workspace as well\n",
        "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/\\\n",
        "large-language-model-pipeline/tune-large-model/v2.0.0'"
      ],
      "metadata": {
        "id": "mCGX6fd5SYhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "date = datetime.datetime.now().strftime(\"%Y%m%d\")"
      ],
      "metadata": {
        "id": "w6-YR--6SqOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = f\"deep-learning-ai-model-{date}\""
      ],
      "metadata": {
        "id": "PR3e3CkUTYVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example uses two PaLM model parameters:\n",
        "\n",
        "*  TRAINING_STEPS: Number of training steps to use when turning the mmodel. For extactive QA you can set it from 100-500.\n",
        "*   EVALUATION_INTERVAL: The interval determines how frequently a trained model is evaluated against the create evaliation set to assess its performance and identify issues. Default will be 20, which means after every 20 training steps, the model is evaliated on the evaluiation dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "fJ2hUW34SzzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_STEPS = 200\n",
        "EVALUATION_INTERVAL = 20"
      ],
      "metadata": {
        "id": "Q3CigMOyStHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load project ID and credentirals\n",
        "from utils import authenticate\n",
        "credentials, PrOJECT_ID = authenticate()"
      ],
      "metadata": {
        "id": "u4Z29OfHVEiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = \"us-central1\""
      ],
      "metadata": {
        "id": "OtCSOnlPVbKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguements, the inputs that goes into the pipeline\n",
        "\n",
        "pipeline_arguements = {\n",
        "    \"model_display_name\": MODEL_NAME,\n",
        "    \"location\": REGION,\n",
        "    \"large_model_reference\": \"text-bison@001\",\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"train_steps\": TRAINING_STEPS,\n",
        "    \"dataset_uri\": TRAINING_DATA_URI,\n",
        "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
        "    \"evaluation_uri\": EVALUATION_DATA_URI,\n",
        "}\n"
      ],
      "metadata": {
        "id": "dERfOGWgVjUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline execution\n",
        "\n",
        "pipeline_root \"./\"\n",
        "\n",
        "job = PipelineJob(\n",
        "    ### path of the yaml file to execute\n",
        "    template_path=template_path,\n",
        "    ### name of the pipeline\n",
        "    display_name=f\"deep_learning-ai_pipeline-{date}\",\n",
        "    ### pipeline arguements (inputs)\n",
        "    parameers_values=pipeline_arguments,\n",
        "    ### region of execution\n",
        "    location=REGION\n",
        "    ### root is where temporary files are being\n",
        "    ### stored by the execution engine\n",
        "    pipeline_root=pipeline_root\n",
        "    ### enable_caching=True will save the outputs\n",
        "    ### of components for re-use, and will only re-run those\n",
        "    ### components for which the code or data has changed\n",
        "    enable_cacjing=True\n",
        ")\n",
        "\n",
        "### Submite for execution\n",
        "job.submit()\n",
        "\n",
        "### check to see the status of the job\n",
        "job.state"
      ],
      "metadata": {
        "id": "4qS2e5beWZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project = PROJECT_ID,\n",
        "              location = REGION,\n",
        "              credentials = credentials)"
      ],
      "metadata": {
        "id": "fCRBdACrcAQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bision@001\")"
      ],
      "metadata": {
        "id": "kWvbfuegcTl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of mulitple models executede and deployed\n",
        "\n",
        "list_tuned_models = model.list_tuned_model_names()\n",
        "\n",
        "for i in list_tined_models:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "QEh5ec0Xcf_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select from one of the endpoints to divide the predcition load\n",
        "\n",
        "import random\n",
        "tuned_model_select = random.choice(list_tuned_models)"
      ],
      "metadata": {
        "id": "1XD-RoOhdS63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Response\n",
        "deployed_model = TextGenerationModel.get_tuned_model\\(tuned_model_select)\n",
        "\n",
        "PROMPT = \"How can I load a csv file using Pandas?\""
      ],
      "metadata": {
        "id": "FDNNg0wodhxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use deployed_model.predict to call the API using the prompt\n",
        "\n",
        "### depending on the latency of your prompt\n",
        "### it can take some time to lad\n",
        "response = deployed_model.predict(PROMPT)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "UQ6rIXmYdyRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Load the first object of the response\n",
        "output = response._prediction_response[0]\n",
        "\n",
        "pprint(output)"
      ],
      "metadata": {
        "id": "XmVyA1IReC3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the second object of the response\n",
        "output = response._prediction_response[0][0]\n",
        "\n",
        "pprint(output)"
      ],
      "metadata": {
        "id": "9WvtufXfhmRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the \"content\" key from the second object\n",
        "final_output = response._prediction_response[0][0]['content']\n",
        "\n",
        "print(final_output)"
      ],
      "metadata": {
        "id": "1-B0KFq5h307"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Management and Templates\n",
        "\n",
        "*   Remember that the model was trained on the data that had an `Instruction` and a `Question` as `Prompt` (Lesson 2)\n",
        "*   In the example, only a `Question` as `Prompt` wass used for a response\n",
        "*   It is important for the production data to be the same as the training data. Difference in data can effect the model performance\n",
        "*   Add the same `Instructuin` as it was used for training data, and combine it with a `Question` to be used as `Prompt`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9a8uFRtmiDok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INSTRUCTION = \"\"\"\\\n",
        "Please answer the following Strackoverflow question on Python. \\\n",
        "Answer it like you are a developer answering Stackoverflow questions.\n",
        "\n",
        "Stackoverflow question:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-EVNGxj3isC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"How can I store my Tensorflow checkpoint on \\\n",
        "Google Cloud Storage? Python\""
      ],
      "metadata": {
        "id": "6W8cRdh-i2ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = f\"\"\"\n",
        "{INSTRUCTION} {QUESTION}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7nwDURqEjVgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PRONOT)"
      ],
      "metadata": {
        "id": "9i84vzz4jcGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_response = deployed_model.predict(PROMPT)\n",
        "output = final_response._prediction_response[0][0]['content']\n",
        "\n",
        "print(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "iwDt2ABUjfyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Safety Attributes\n",
        "*   The response also includes saftery scores\n",
        "*   These scores can be used to make sure that LLM's resposnse is within the boundaries of the expected behaviour\n",
        "*   The first layer for this check, is by the model itself\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hEZ5p5Otjrqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the 'blocked' key from the 'safteyAttrributes' of the respoinse\n",
        "blocked = response._prediction_response[0]['safetyAttributes']['blocked']\n",
        "\n",
        "print(blocked)"
      ],
      "metadata": {
        "id": "nnvS_ikajp8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if any response was blocked\n",
        "print(blocked)"
      ],
      "metadata": {
        "id": "8n9JS2YEkTxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "safety_attributes = response._prediction_response[0][0]\\\n",
        "['safetyAttributes']\n",
        "\n",
        "pprint(safety_attributes)"
      ],
      "metadata": {
        "id": "hh3hgIDKkZ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the 'citations' key from the 'citationsMetadata' of the response\n",
        "citation = response._prediction_response[0][0]\\\n",
        "['citiationMetaData']['citaitions']\n",
        "\n",
        "pprint(citation)"
      ],
      "metadata": {
        "id": "C-5x7ECAkrOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"Finish the sentence: to be, or not \""
      ],
      "metadata": {
        "id": "cqgpLmdolLM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = deployed_model.predict(PROMPT)\n",
        "\n",
        "output = response._prediction_response[0][0]['content']\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "p2igOxtPlQqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for citation\n",
        "citation = response._prediction_response[0][0]\\\n",
        "['citiationMetadata']['citiations']\n",
        "\n",
        "pprint(citation)"
      ],
      "metadata": {
        "id": "NLlQl--klaQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}